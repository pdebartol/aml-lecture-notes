\documentclass[twoside]{article}
\setlength{\oddsidemargin}{0.25 in}
\setlength{\evensidemargin}{-0.25 in}
\setlength{\topmargin}{-0.6 in}
\setlength{\textwidth}{6.5 in}
\setlength{\textheight}{8.5 in}
\setlength{\headsep}{0.75 in}
\setlength{\parindent}{0 in}
\setlength{\parskip}{0.1 in}
\newcommand{\eqdef}{:\mathrel{\mathop=}}
\newcommand{\norm}[1]{\left\lVert #1 \right\rVert}

%
% ADD PACKAGES here:
%
\usepackage{algorithm}
\usepackage[noend]{algpseudocode}
\usepackage{amsmath,amsfonts,graphicx}
\usepackage{amssymb}

%
% The following commands set up the lecnum (lecture number)
% counter and make various numbering schemes work relative
% to the lecture number.
%
\newcounter{lecnum}
\renewcommand{\thepage}{\thelecnum-\arabic{page}}
\renewcommand{\thesection}{\thelecnum.\arabic{section}}
\renewcommand{\theequation}{\thelecnum.\arabic{equation}}
\renewcommand{\thefigure}{\thelecnum.\arabic{figure}}
\renewcommand{\thetable}{\thelecnum.\arabic{table}}

%
% The following macro is used to generate the header.
%
\newcommand{\lecture}[4]{
   \pagestyle{myheadings}
   \thispagestyle{plain}
   \newpage
   \setcounter{lecnum}{#1}
   \setcounter{page}{1}
   \noindent
   \begin{center}
   \framebox{
      \vbox{\vspace{2mm}
    \hbox to 6.28in { {\bf Advanced Machine Learning
	\hfill Fall 2021} }
       \vspace{4mm}
       \hbox to 6.28in { {\Large \hfill Lecture #1: #2  \hfill} }
       \vspace{2mm}
       \hbox to 6.28in { {\it  #3 \hfill  #4} }
      \vspace{2mm}}
   }
   \end{center}
   \markboth{Lecture #1: #2}{Lecture #1: #2}

   {\bf Note}: {\it LaTeX template courtesy of UC Berkeley EECS dept.}

   {\bf Disclaimer}: {\it These notes are adapted from ETH's Advanced Machine Learning Course,  Bishop's "Pattern Recognition and Machine Learning" book and Joachims et al.'s paper: "Predicting Structured Objects with Support Vector Machines"}
   \vspace*{4mm}
}
%
% Convention for citations is authors' initials followed by the year.
% For example, to cite a paper by Leighton and Maggs you would type
% \cite{LM89}, and to cite a paper by Strassen you would type \cite{S69}.
% (To avoid bibliography problems, for now we redefine the \cite command.)
% Also commands that create a suitable format for the reference list.
\renewcommand{\cite}[1]{[#1]}
\def\beginrefs{\begin{list}%
        {[\arabic{equation}]}{\usecounter{equation}
         \setlength{\leftmargin}{2.0truecm}\setlength{\labelsep}{0.4truecm}%
         \setlength{\labelwidth}{1.6truecm}}}
\def\endrefs{\end{list}}
\def\bibentry#1{\item[\hbox{[#1]}]}

%Use this command for a figure; it puts a figure in wherever you want it.
%usage: \fig{NUMBER}{SPACE-IN-INCHES}{CAPTION}
\newcommand{\fig}[3]{
			\vspace{#2}
			\begin{center}
			Figure \thelecnum.#1:~#3
			\end{center}
	}
% Use these for theorems, lemmas, proofs, etc.
\newtheorem{theorem}{Theorem}[lecnum]
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{proposition}[theorem]{Proposition}
\newtheorem{claim}[theorem]{Claim}
\newtheorem{corollary}[theorem]{Corollary}
\newtheorem{definition}[theorem]{Definition}
\newenvironment{proof}{{\bf Proof:}}{\hfill\rule{2mm}{2mm}}

% **** IF YOU WANT TO DEFINE ADDITIONAL MACROS FOR YOURSELF, PUT THEM HERE:

\newcommand\E{\mathbb{E}}

\begin{document}
%FILL IN THE RIGHT INFO.
%\lecture{**LECTURE-NUMBER**}{**DATE**}{**LECTURER**}{**SCRIBE**}
\lecture{8}{Structured SVMs }{}{}
%\footnotetext{These notes are partially based on those of Nigel Mansell.}

% **** YOUR NOTES GO HERE:

% Some general latex examples and examples making use of the
% macros follow.  
%**** IN GENERAL, BE BRIEF. LONG SCRIBE NOTES, NO MATTER HOW WELL WRITTEN,
%**** ARE NEVER READ BY ANYBODY.


\section{Non-Linear SVMs}
The SVM soft-margin dual formulation is the following:
\begin{equation*}
\begin{aligned}
& \underset{a}{\text{max}}
& & \sum_{i=1}^{N} a_{i}  - \dfrac{1}{2}
    \sum_{i=1}^{N} { \sum_{j=1}^{N} a_{i} a_{j} t_{i} t_{j} \phi(x_{i})^T \phi(x_{j})}\\
& \text{subject to}
& & 0 \leq a_{i} \leq C  , \; i = 1, \ldots, N \\
&&& \sum_{i=1}^{N} a_{i}t_{i} = 0
\end{aligned}
\end{equation*}
For very high dimensional space, the nonlinear transformation $\phi(x)$ might be
computationally too difficult to compute, when we only require the inner product $\langle \phi(x_{i}),\, \phi(x_{j}) \rangle$. A way to overcome this problem is making use of \textbf{Kernel} functions.


\begin{definition}
A function $k : \mathbb{R}^d \times \mathbb{R}^d \rightarrow \mathbb{R}$ is said to be a kernel function if and only if it is an inner product $\langle \boldsymbol{\phi(x)}, \boldsymbol{\phi(x')} \rangle$  for some (possibly infinite dimensional) mapping $\boldsymbol{\phi(x)}$.
\end{definition}
\newpage
\begin{definition}
A function $k : \mathbb{R}^d \times \mathbb{R}^d \rightarrow \mathbb{R}$ is said to be symmetric positive semidefinite (PSD) if it is symmetric, i.e \: $k(\boldsymbol{x}, \boldsymbol{x'}) = k(\boldsymbol{x'}, \boldsymbol{x})$; For any integer $m > 0$ and any set of inputs $\boldsymbol{x_1,...,x_m }\in \mathbb{R}^d$, the following matrix is positive semi-definite:
\begin{center}
    $\boldsymbol{K} =
    \begin{bmatrix}
    k(\boldsymbol{x_1}, \boldsymbol{x_1}) & \hdots & k(\boldsymbol{x_1}, \boldsymbol{x_m})\\
    \vdots & \ddots & \vdots\\
    k(\boldsymbol{x_m}, \boldsymbol{x_1}) & \hdots & k(\boldsymbol{x_m}, \boldsymbol{x_m})
    \end{bmatrix}
    \succeq \boldsymbol{0}
$
\end{center}
This matrix, with $(i, j)$-th entry equal to $k(\boldsymbol{x_i}, \boldsymbol{x_j})$, is called the \textbf{Gram matrix}.
\end{definition}

\begin{theorem}
The above two definitions are equivalent. That is, $k$ is a kernel function if and only if it is symmetric PSD.
\end{theorem}
\begin{proof}
The “only if” part is easy to show (at least when $\phi$ is finite-dimensional): The inner product is certainly symmetric, and the Gram matrix can be written as $\boldsymbol{K = \Phi^\intercal\Phi}$, where $\boldsymbol{\Phi} \in \mathbb{R}^{\text{dim}(\boldsymbol{\Phi}) \times m}$ contains the $m$ feature vectors $\{\boldsymbol{\phi(x_t)}\}_{t = 1}^{m}$ as columns. The matrix $\boldsymbol{K = \Phi^\intercal\Phi}$  is trivially positive semidefinite, since for any $\boldsymbol{z}$ we have $\boldsymbol{z^\intercal\Phi^\intercal\Phi z} = \boldsymbol{\norm{\Phi z}^2} \geq 0$.\medskip

The “if” part is more challenging and comes from \textit{Mercer's Theorem}.
\end{proof}\medskip

\begin{theorem}{\textbf{Mercer's  Theorem}\\}
Recall: any positive definite matrix $\boldsymbol{K}$ can be represented using an eigendecomposition of the form $\boldsymbol{K = U^\intercal\Lambda U}$, where $\boldsymbol{\Lambda}$ is a diagonal matrix of eigenvalues $\lambda_i > 0$, and $\boldsymbol{U}$ is a matrix containing the eigenvectors.\\
Now consider element $(i, j)$ of $\boldsymbol{K}$:
\begin{equation*}
    k_{ij} = (\boldsymbol{\Lambda}^\frac{1}{2}\boldsymbol{U}_{:i})^\intercal(\boldsymbol{\Lambda}^\frac{1}{2}\boldsymbol{U}_{:j})
\end{equation*}
where $\boldsymbol{U}_{:i}$ is the \textit{i}'th column of $\boldsymbol{U}$. If we define $\boldsymbol{\phi(x_i)} = \boldsymbol{\Lambda}^\frac{1}{2}\boldsymbol{U}_{:i}$, then we can write:
\begin{equation*}
    k_{ij} = \boldsymbol{\phi(x_i)}^\intercal \boldsymbol{\phi(x_j)} = \sum\limits_m \phi_m (\boldsymbol{x_i})\phi_m(\boldsymbol{x_j})
\end{equation*}
\end{theorem}
Thus we see that the entries in the kernel matrix can be computed by performing an inner product of some feature vectors that are implicitly defined by the eigenvectors of the kernel matrix. This idea can be generalized to apply to kernel functions, not just kernel matrices.\\
For example consider the \textbf{quadratic kernel} $\mathcal{K}(\boldsymbol{x}, \boldsymbol{x'}) = \langle\boldsymbol{x}, \boldsymbol{x'}\rangle^2$. In 2d, we have:
\begin{equation*}
    \mathcal{K}(\boldsymbol{x}, \boldsymbol{x'}) = (x_1x_1' + x_2x_2')^2 = x_1^2(x_1')^2 + 2x_1x_2x_1'x_2' + x_2^2(x_2')^2
\end{equation*}
We can write this as $\mathcal{K}(\boldsymbol{x}, \boldsymbol{x'}) = \boldsymbol{\phi(x)}^\intercal\boldsymbol{\phi(x)}$ if we define $\boldsymbol{\phi(x_1, x_2)} = [x_1^2, \sqrt{2}x_1x_2, x_2^2] \in \mathbb{R}^3$. So we embed the 2d inputs $\boldsymbol{x}$ into a 3d feature space $\boldsymbol{\phi(x)}$.\\
Now consider the RBF kernel. In this case, the corresponding feature representation is infinite dimensional. However, by working with kernel functions, we can avoid having to deal with infinite dimensional vectors.
\newpage
\subsection{Kernel Engineering}
Given two valud kernels $\mathcal{K}_1(\boldsymbol{x}, \boldsymbol{x'})$ and $\mathcal{K}_2(\boldsymbol{x}, \boldsymbol{x'})$, we can create a new kernel using any of the following methods:
\begin{itemize}
    \item $\mathcal{K}(\boldsymbol{x}, \boldsymbol{x'}) = c\mathcal{K}_1(\boldsymbol{x}, \boldsymbol{x'})$, for any constant $c > 0$
    \item $\mathcal{K}(\boldsymbol{x}, \boldsymbol{x'}) = f(\boldsymbol{x})\mathcal{K}_1(\boldsymbol{x}, \boldsymbol{x'})f(\boldsymbol{x'})$, for any function $f$
    \item $\mathcal{K}(\boldsymbol{x}, \boldsymbol{x'}) = q(\mathcal{K}_1(\boldsymbol{x}, \boldsymbol{x'}))$, for any function polynomial $q$ with non-negative coefficients
    \item $\mathcal{K}(\boldsymbol{x}, \boldsymbol{x'}) = \exp{(\mathcal{K}_1(\boldsymbol{x}, \boldsymbol{x'}))}$
    \item $\mathcal{K}(\boldsymbol{x}, \boldsymbol{x'}) = \boldsymbol{x}^\intercal\boldsymbol{Ax'}$, for any PSD matrix $\boldsymbol{A}$
\end{itemize}
For example, suppose we start with the linear kernel $\mathcal{K}(\boldsymbol{x}, \boldsymbol{x'}) = \boldsymbol{x^\intercal x'}$. We know this is a valid Mercer kernel, since the corresponding Gram matrix is just the (scaled) covariance matrix of the data. From the above rules, we can see that the polynomial kernel $\mathcal{K}(\boldsymbol{x}, \boldsymbol{x'}) = (\boldsymbol{x^\intercal x'})^M$is a valid Mercer kernel. This contains all monomials of order M. For example, if M = 2 and the inputs are 2d, we have:
$$(\boldsymbol{x^\intercal x'})^2 = (x_1x_1' + x_2x_2')^2 = (x_1x_1')^2 + (x_2x_2)^2 + (x_1x_1')(x_2x_2')$$
We can generalize this to contain all terms up to degree M by using the kernel $\mathcal{K}(\boldsymbol{x}, \boldsymbol{x'}) = (\boldsymbol{x^\intercal x'} + c)^M$ . For example, if M = 2 and the inputs are 2d, we have:
$$(\boldsymbol{x^\intercal x'} + 1)^2 = (x_1x_1')^2 + (x_1x_1')(x_2x_2') + (x_1x_1') + (x_2x_2)(x_1x_1') + (x_2x_2')^2 + (x_2x_2') + (x_1x_1') + (x_2x_2') + 1$$
We can also use the above rules to establish that the Gaussian kernel is a valid kernel. To see this,
note that:
$$\norm{\boldsymbol{x} - \boldsymbol{x'}}^2 = \boldsymbol{x}^\intercal\boldsymbol{x} + (\boldsymbol{x'})^\intercal\boldsymbol{x'} - 2\boldsymbol{x}^\intercal\boldsymbol{x'}$$
and hence
$$\mathcal{K}(\boldsymbol{x}, \boldsymbol{x'}) = \exp{(-\norm{\boldsymbol{x} - \boldsymbol{x'}}^2/2\sigma^2)} = \exp{(-\boldsymbol{x}^\intercal\boldsymbol{x}/2\sigma^2)}\exp{(\boldsymbol{x}^\intercal\boldsymbol{x'}/\sigma^2)}\exp{(-(\boldsymbol{x'})^\intercal\boldsymbol{x'}/2\sigma^2)}$$
is a valid kernel.
\subsection{Combining kernels by addition and multiplication}
We can also combine kernels using addition or multiplication:
\begin{itemize}
    \item $\mathcal{K}(\boldsymbol{x}, \boldsymbol{x'}) = \mathcal{K}_1(\boldsymbol{x}, \boldsymbol{x'}) + \mathcal{K}_2(\boldsymbol{x}, \boldsymbol{x'})$
    \item $\mathcal{K}(\boldsymbol{x}, \boldsymbol{x'}) = \mathcal{K}_1(\boldsymbol{x}, \boldsymbol{x'}) \times \mathcal{K}_2(\boldsymbol{x}, \boldsymbol{x'})$
\end{itemize}
Multiplying two positive-definite kernels together always results in another positive definite kernel. This is a way to get a conjunction of the individual properties of each kernel. In addition, adding two positive-definite kernels together always results in another positive definite
kernel. This is a way to get a disjunction of the individual properties of each kernel.
\subsection{Kernels for structured inputs}
Kernels are particularly useful when the inputs are structured objects, such as strings and graphs, since it is often hard to “featurize” variable-sized inputs. For example, we can define a string kernel which compares strings in terms of the number of n-grams they have in common.
We can also define kernels on graphs . For example, the random walk kernel conceptually performs random walks on two graphs simultaneously, and then counts the number of paths that were produced by both walks.



\section{Structured SVMs}
Consider the problem of natural language parsing illustrated in Figure 8.2. A parser takes as input a natural
language sentence, and the desired output is the parse tree
decomposing the sentence into its constituents. How can we take, say,
an SVM and learn a rule for predicting trees?

\begin{figure}[h]
\caption{Predicting trees in natural language parsing.}
\centering
\includegraphics[width=0.39\textwidth]{img/syn_tree.png}
\end{figure}


Obviously, this question arises not only for learning to
predict trees, but similarly for a variety of other structured
and complex outputs. Structured output prediction is the
name for such learning tasks, where one aims at learning
a function $h : X \rightarrow Y$ mapping inputs $x \in X$ to complex
and structured outputs $y \in Y$. 
\\ \\ On an abstract level, a structured prediction task is much like
a multi-class learning task. Each possible structure $y \in Y$
(e.g. parse tree) corresponds to one class (see Figure 8.3), and
classifying a new example $x$ amounts to predicting its correct
“class”. 

\begin{figure}[h]
\caption{Structured output prediction as a multiclass problem.}
\centering
\includegraphics[width=0.39\textwidth]{img/syn_out.png}
\end{figure}

While the following derivation of structural SVMs
starts from multi-class SVMs, there are three key problems
that need to be overcome. \textbf{All of these problems arise from
the huge number $\mathbf{|Y|}$ of classes}. 
\newpage

\subsection{Problem 1: Structural SVM Formulation}
We start the derivation of the structural SVM from the multi-class SVM. These multi-class SVMs use one weight vector $\mathbf{w_{y}}$ for each class $\mathbf{y}$. Each input
$\mathbf{x}$ now has a score for each class $\mathbf{y}$ via $\mathbf{f(x,y) = w_{y} \phi(x)}$.
Here $\mathbf{\phi(x)}$ is a vector of binary or numeric features extracted
from $\mathbf{x}$. Thus, every feature will have an additively weighted
influence in the modeled compatibility between inputs $\mathbf{x}$ and
classes $\mathbf{y}$. To classify $\mathbf{x}$, the prediction rule $\mathbf{h(x)}$ then simply
chooses the highest-scoring class:

\begin{equation}
    h(x) = \max_{y \in Y} f(x,y)
\end{equation}

as the predicted output. This will result in the correct prediction $\mathbf{y}$ for input $\mathbf{x}$ provided the weights $\mathbf{w = (w_{1}, . . . , w_{k})}$
have been chosen such that the inequalities $\mathbf{f(x,y_{i}) < f(x, y)}$
hold for all incorrect outputs $\mathbf{y_{i} \not =  y}$. \\

\begin{equation}
\begin{aligned}
& \underset{\textbf{w}}{\text{min}}
& & \dfrac{1}{2} \norm{w}^2  \\
& \text{subject to}
& & f(x_{i},y_{i}) - f(x_{i},y) = (w_{y_i} - w_y)\cdot \phi(x_i) \geq 1 , \;\forall i, y \not = y_{i}
\end{aligned}
\end{equation}

The first challenge in using (8.8) for structured outputs is that, while there is generalization across inputs $\mathbf{x}$, there is no generalization across outputs. This is due to having a
separate weight vector $\mathbf{w_y}$ for each class $\mathbf{y}$. Furthermore,
since the number of possible outputs can become very large (or infinite), naively reducing structured output prediction
to multi-class classification leads to an undesirable blow-up
in the overall number of parameters and in the overall number of inequalities, which is $\mathbf{n(k-1)}$. \\

The key idea in overcoming these problems is to extract
features from input-output pairs using a so-called joint feature map $\mathbf{\Psi(x, y)}$ instead of $\mathbf{\Phi(x)}$. These joint features will allow us to generalize across outputs and to define meaningful scores even
for outputs that were never actually observed in the training data. At the same time, since we will define compatibility
functions via $\mathbf{f(x, y) = w\cdot \Psi(x, y)}$, the number of parameters will simply equal the number of features extracted via
$\mathbf{\Psi}$, which may not depend on $\mathbf{|Y|}$. One can then use the formulation in (8.8) with the more flexible definition of $\mathbf{f}$ via $\mathbf{\Psi}$ to
arrive at the following (hard-margin) optimization problem:

\begin{equation*}
\begin{aligned}
& \underset{\textbf{w}}{\text{min}}
& & \dfrac{1}{2} \norm{w}^2  \\
& \text{subject to}
& & w \cdot \Psi(x_{i},y_{i}) - w \cdot \Psi(x_{i},y) \geq 1 , \;\forall i, y \not = y_{i}
\end{aligned}
\end{equation*}

In other words, find a weight vector $\mathbf{w}$ of an input-output compatibility function $\mathbf{f}$ that is linear in some joint feature map $\mathbf{\Psi}$
so that on each training example it scores the correct output
higher by a fixed margin than every alternative output, while
having low complexity (i.e. small norm $\norm{w}$). Note that the
number of linear constraints is still $n(|Y| -1)$. The design of the features $\mathbf{\Psi}$ is problem-specific, and it is a
strength of the developed methods to allow for a great deal of flexibility in how to choose it.

\subsection{Problem 2: Inconsistent Training Data}
So far we have tacitly assumed that the optimization problem has a solution, i.e. there exists a weight vector
that simultaneously fulfills all margin constraints. In practice this may not be the case, either because the training
data is inconsistent or because our model class is not powerful enough. If we allow for mistakes, though, we must
be able to quantify the degree of mismatch between a prediction and the correct output, since usually different incorrect predictions vary in quality. This is exactly the role
played by a loss function, formally $ \Delta : Y \times Y \rightarrow \mathbb{R}$, where
$\mathbf{\Delta(y_{i},y)}$ is the loss (or cost) for predicting $\mathbf{y}$, when the correct output is $\mathbf{y_{i}}$. Like the choice of $\mathbf{\Psi}$, defining $\mathbf{\Delta}$
is problem-specific. One can convert this back into a
quadratic program as follows:

\begin{equation*}
\begin{aligned}
& \underset{\textbf{w}, \xi_{i} \geq 0}{\text{min}}
& & \dfrac{1}{2} \norm{w}^2 + \dfrac{C}{n} \sum_{i=1}^{n} \xi_{i} \\
& \text{subject to}
& & w \cdot \Psi(x_{i},y_{i}) - w \cdot \Psi(x_{i},y) \geq  \Delta(y_{i},y) - \xi_{i} , \;\forall i, y \not = y_{i}
\end{aligned}
\end{equation*}

Note that we added the $\mathbf{\dfrac{1}{n}}$ term, this is a guarantee that we are minimizing the empirical risk:

\begin{theorem}
If $w^*,\xi^*$ are optimal, then the empirical risk of $w^*$ with respect to $\mathbf{\Delta}$:
\begin{equation*}
    \dfrac{1}{n} \sum_{i=1}^{n}\Delta(y_{i},h_{w^*}(x_{i})) \leq \dfrac{1}{n} \sum_{i=1}^{n} \xi_{i}^*
\end{equation*}
\end{theorem}
\begin{proof}
Suffices to prove that  $\Delta(y_{i},h_{w^*}(x_{i})) \leq \xi_{i}^*,  \; \forall i$ \\

If $h_{w^*}(x_{i}) = y_{i}$, then $\Delta(y_{i},h_{w^*}(x_{i})) = 0 \leq \xi_{i}^*$ \\

If $h_{w^*}(x_{i}) = y \not = y_{i}$, then:
\begin{align*}
       w^{*T}\cdot \Psi(x_{i},y_{i}) < w^{*T} \cdot \Psi(x_{i},y) \implies w^{*T}\cdot \Psi(x_{i},y_{i}) - w^{*T} \cdot \Psi(x_{i},y) = \delta < 0\\      \delta + \xi_{i}^* \geq \Delta(y_{i},y)  \implies  \xi_{i}^* \geq \Delta(y_{i},y)  , \;\forall i \; \text{(from the optimization constraint)}
\end{align*}
\end{proof}


\subsection{Problem 3: Efficient Training}

Last, but not least, we need a training algorithm that finds
the optimal $\mathbf{w}$ solving the quadratic program. Since
there is a constraint for every incorrect label $\mathbf{y}$, we cannot
enumerate all constraints and simply give the optimization problem to a standard QP solver. Instead, we propose to use the cutting-plane Algorithm 1. The key idea is to iteratively construct
a working set of constraints $\mathbf{W}$ that is equivalent to the full
set of constraints  up to a specified precision $\mathbf{\epsilon}$.

\begin{figure}[h]
\centering
\includegraphics[width=0.52\textwidth]{img/struct_algo.png}
\end{figure}


Starting
with an empty $\mathbf{W}$ and $\mathbf{w=0}$, Algorithm 1 iterates through
the training examples. For each example, the $\mathbf{argmax}$ in Line 5 finds the most violated constraint of the quadratic
program. If this constraint is violated by more than $\mathbf{\epsilon}$ (Line 6), it is added to the working set $\mathbf{W}$ in Line 7 and a
new $\mathbf{w}$ is computed by solving the quadratic program over
the new $\mathbf{W}$ (Line 8). The algorithm stops and returns the
current $\mathbf{w}$ if $\mathbf{W}$ did not change between iterations.\\ 

\textbf{But how long does
it take to terminate?} It can be shown that Algorithm 1
always terminates in a polynomial number of iterations that is independent of the cardinality of the output space $|Y|$. In
fact, a refined version of Algorithm 1  always terminates after adding at most $O(C\epsilon^{-1})$ constraints to $W$. Note that the number of constraints
is not only independent of $|Y|$, but also independent of the
number of training examples $n$, which makes it an attractive
training algorithm even for conventional SVMs. \\
While the number of iterations is small, the $\mathbf{argmax}$ in
Line 5 might be expensive to compute. In general, this is
true, but note that this $\mathbf{argmax}$ is closely related to the
$\mathbf{argmax}$ for computing a prediction $\mathbf{h(x)}$. It is therefore
called the “loss-augmented”inference problem, and often the
prediction algorithm can be adapted to efficiently solve the
loss-augmented inference problem as well.

\end{document}
